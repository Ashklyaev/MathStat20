\documentclass[10 pt,russian]{report}
\renewcommand{\baselinestretch}{1}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{mathrsfs}
\usepackage{graphics}
\geometry{verbose,letterpaper,tmargin=0.5 cm,bmargin=0.8 cm,lmargin=1cm,rmargin=1cm,headsep=1cm}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{10 pt}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{cmap}
\usepackage{textcomp}
\usepackage{setspace}
\usepackage{indentfirst}
\usepackage[russian]{babel}
\begin{document}
\newtheorem{Th}{Теорема}
\newtheorem{Lemm}{Лемма}
\theoremstyle{definition}
\newtheorem{Def}{Определение}
\newtheorem{Rem}{Замечание}
\newtheorem{Exam}{Пример}
\newtheorem{Que}{Вопрос}
\thispagestyle{empty}
\def\chaptername{Семинар}
\sloppy
\large
%Сделать графу %самые важные вещи, которые я должен вынести из семинара
\chapter{Оценки. Несмещенность. Квадратичное отклонение}
\section{Базовая часть}
\subsection{Точечное оценивание}
Считается, что вы уже ознакомились с введением, в котором описывалась логика построения статистической модели. Здесь же я непосредственно буду пользоваться этой моделей без дополнительного пояснения.

Пусть $(\mathcal{X}, \mathbb{F}, \{{\bf P}_{\theta}(\cdot),\theta\in \Theta\})$ -- статистическое пространство, $\Theta\subseteq \mathbb{R}^k$, $\mathcal{X}\subseteq\mathbb{R}^m$, $k$, $m$ -- некоторые константы.

Мы рассматриваем случайный вектор $\vec{X}$ со значениями в $\mathcal{X}$, имеющий распределение ${\bf P}_{\theta}$ при некотором $\theta\in \Theta$. Задача точечного оценивания предполагает построение измеримой (борелевской) функции $\widehat{\theta}$ из $\mathcal{X}$ в $\mathbb{R}^k$, которую мы называем оценкой. Мы ожидаем, что $\widehat{\theta}(\vec{X})$ в некотором смысле близка к функции $h(\theta)$, которую мы хотим оценить.

Какими свойствами должна обладать наша оценка? Что означает ''близка''? Мы рассмотрим несколько возможных вариантов и подходов.

Отметим, что приведенные выше соображения относятся к параметрическому случаю, с которым мы преимущественно будем работать. В непараметрическом случае мы рассматриваем пространство $(\mathcal{X},\mathcal{F},\mathcal{P})$, где $\mathcal{P}$ -- непараметризованное множество мер. При этом мы также строим оценку $\widehat{\theta}$, но уже не для функции $h(\theta)$, а для параметра распределения $h(P)$, где $h:\mathcal{P}\to \mathbb{R}^k$ -- некоторое отображение из множества мер в множество векторов. 
\subsection{Точечное оценивание}
Итак, пусть у нас есть выборка $\vec{X}$ из распределения ${\bf P}_{\theta}$. Мы бы хотели построить функцию $\widehat{\theta}(\vec{X})$, которая бы хорошо оценивала $\theta$. Для начала стоит задаться хрестоматийным вопросом --- ''Что такое хорошо?''
\begin{Def}
Оценка $\widehat{\theta}$ называется несмещенной для функции $g(\theta)$, если ${\bf E}_{\theta} \widehat{\theta}(X) = g(\theta)$ при всех $\theta$.
\end{Def}
\begin{Def}
Смещением оценки $\widehat{\theta}$ называют величину ${\bf E}_{\theta}\widehat{\theta}(X)-g(\theta)$. 
\end{Def}
Здесь и далее ${\bf E}_{\theta}$ обозначает математическое ожидание по мере ${\bf P}_{\theta}$. 

Будем говорить, что оценка несмещенная без указания функции, если подразумевается, что $g(\theta)=\theta$.

Аналогичным образом в непараметрической модели оценка называется несмещенной для параметра распределения $h(P)$, если
$${\bf E}_{P} \widehat{\theta}(\vec{X}) = h(P), \quad P\in \mathcal{P}.$$
Здесь ${\bf E}_P$ -- математическое ожидание по мере $P$.

В дальнейшем мы будем рассматривать случай, когда выборка представляет собой вектор из н.о.р. величин $X_1,\dotsc, X_n$. Соответственно, многомерное распределение ${\bf P}_{\theta}$ мы будем задавать одномерным распределением $X_1$.

\Exam Предположим, что на основе неких измерений в течение дня, мы делаем вывод о необходимых закупках товара в магазин. Несмещенность, в силу закона больших чисел, гарантирует, что средняя закупка за $N$ дней будет стремиться c  ростом $N$ к искомому параметру. Смещенные оценки будут иметь систематический снос, то мы есть будем систематически недокупать (перекупать) товар. 

С другой стороны, сама по себе несмещенность не обеспечивает ''разумности'' оценки.
\Exam Рассмотрим схему Бернулли с неизвестным параметром $\theta$ и рассмотрим оценку $\overline{X} = (X_1+...+X_n)/n$. Иначе говоря, чтобы оценить вероятность успеха, мы делим число успехов на число испытаний, что вполне естественно. В силу линейности математического ожидания, эта оценка несмещенная. При этом можно взять оценку $X_1$. Оценка несмещена, поскольку ${\bf E}_{\theta} X_1 = \theta$, но явно не слишком-то удобна для использования. Как минимум, она принимает только значения 0 и 1 и зависит лишь от первого элемента выборки.

\Exam Собственно, даже в общей непараметрической модели оценка $\overline{X}$ будет несмещенной для ${\bf E}_P X$,  где $\mathcal{P}=\{P: {\bf E}_P X<\infty\}$. Действительно, 
$${\bf E}_{P} \overline{X}=  \frac1n {\bf E}_P (X_1+\dotsc +X_n)= {\bf E}_P X_1.
$$

Поэтому в любой модели эта оценка является несмещенной для математического ожидания. Такая оценка широко используется и носит название {\it выборочного среднего}. 

Будем обозначать $\overline{f(X)}$ величину $n^{-1} \sum_{i=1}^{n} f(X_i)$. Аналогичным образом эта оценка несмещена для ${\bf E}_P f(X)$, если это математическое ожидание конечно.

\Exam Поскольку
$$
h(P) = {\bf P}(X\le x) = {\bf E}_{P} I_{X\le x},
$$
то в рамках предыдущего примера мы можем использовать несмещенную оценку
$$
\widehat{F}_n(x) = \overline{I_{X\le x}} = \frac1n \sum_{i=1}^{n} I_{X_i\le x}
$$
для оценки ф.р. наших наблюдений. Эта оценка играет большую роль в непараметрической статистике и называется эмпирической функцией распределения.

\Exam А как оценить дисперсию? Для этого используют оценку, которую называют {\it выборочной дисперсией}:
$$
S^2:= \overline{(X-\overline{X})^2} = \frac1n \sum_{i=1}^{n} (X_i- \overline{X})^2 = \overline{X^2} - \overline{X}^2.
$$
Эта оценка не будет несмещенной, но ее легко исправить так, что она станет несмещенной.
\subsection{Построение несмещенных оценок}
Отметим существенную проблему, усложняющую построение несмещенных оценок --- если ${\bf E}\widehat{\theta}(X_1,...,X_n) = g(\theta)$, то 
${\bf E}h(\widehat{\theta}(X_1,...,X_n))$, вообще говоря, не равно $h(g(\theta))$. Более того, для строго выпуклых $h$ и невырожденных (то есть непостоянных) случайных величин $Y$  верно неравенство Иенсена
$$
{\bf E}h(Y) > h({\bf E} Y).
$$

Тем самым, построение в каждой модели несмещенных оценок требует отдельных расчетов для различных функций $g(\theta)$. Другие свойства, которые мы пройдем на следующих занятиях, зачастую более просты для исследования. Более того, их может и вовсе не оказаться.

\Exam Существует ли несмещенная оценка для $1/\theta$ на основе одного наблюдения $X\sim R[0,\theta]$, $\theta>0$? Пусть да. Тогда
$$
{\bf E}_{\theta}\widehat{\theta}(X) = \int_0^{\theta} \frac{1}{\theta} \widehat{\theta}(x) dx = \frac1{\theta}.
$$
Значит, при всех $\theta$ получаем 
$$
\int_0^{\theta} \widehat{\theta}(x) dx = 1
$$
при всех $\theta>0$. Но левый интеграл стремится к нулю при $\theta\to 0$, а правый нет. Противоречие показывает, что несмещенных оценок не существует.

\Exam Отметим, что если в той же задаче рассмотреть $\Theta = (1,\infty)$, то несмещенной оценкой будет $I_{X\in [0,1]}$. Это важное замечание -- существование несмещенной оценки существенно зависит от области изменения параметра.

Зачастую можно искать несмещенную оценку подбором, пользуясь известными оценками.
\Exam
\label{ExExp}
 Построить несмещенную оценку для $\theta^{-2}$ на основе $X_i\sim \exp(\theta)$.\\
Математическое ожидание ${\bf E}_{\theta}X_1=\theta^{-1}$, поэтому можно предположить, что $\overline{X}$ хорошая оценка для $\theta^{-1}$, а $\overline{X}^2$ для $\theta^{-2}$. Но если ${\bf E}_{\theta}\overline{X}=\theta^{-1}$, то ${\bf E}_{\theta}\overline{X}^2>\theta^{-2}$, поскольку ${\bf E}Y^2>({\bf E}Y)^2$ для любой непостоянной $Y$.\\
Сможем ли мы исправить смещение $\overline{X}^2$? Заметим, что $\sum_{i=1}^{n} X_i\sim \Gamma\left(n,\frac1{\theta}\right)$, поэтому 
$${\bf E}\overline{X}^2 = \frac1{n^2 \Gamma(n)} \int_{0}^{\infty} x^2  x^{n-1}e^{-\frac{x}{\theta}} dx = \frac{\Gamma(n+2)}{\Gamma(n) n^2}\theta^2 = \frac{(n+1)n}{n^2}\theta^2 = \frac{n+1}{n} \theta^2.$$


\Exam Построим несмещенную оценку для $e^{-\theta}$ для выборки с распределение $Poiss(\theta)$.\\
Найдем оценку, зависящую только от одного наблюдения, $\widehat{\theta}(X_1) = h(X_1)$. Тогда
$${\bf E}_{\theta} h(X_1) = \sum_{k=0}^{\infty} h(k) \frac{e^{-\theta} \theta^k}{k!} = e^{-\theta},$$
то есть
$$\sum_{k=0}^{\infty} \frac{h(k) \theta^k}{k!} = 1.$$
Легко угадать решение --- $h(k)=I_{k=0}$. Задумавшись, вы легко поймете почему это единственное решение и как найти его, не угадывая с помощью ряда Тейлора.
Формально оценка $I_{X_1=0}$ --- уже решение задачи, но давайте построим более качественную оценку
$$\overline{I_{X=0}} = \frac1n \sum_{i=1}^{n} I_{X_i=0},$$
у которой то же математическое ожидание, но которая гораздо разумнее как оценка (как минимум, она зависит от всех наблюдений, а не только от первого).
\subsection{Квадратичное отклонение}
Измерять отклонение оценки от оцениваемой функции $g(\theta)$ естественно оценивать квадратичным отклонением
$$
{\bf E}_{\theta}(\widehat{\theta}\left(\vec{X})-g(\theta)\right)^2.
$$
Для несмещенных оценок данная величина есть
$$
{\bf D}_{\theta} \widehat{\theta}(\vec{X}).
$$
Для произвольных оценок нетрудно вывести аналогичный факт:
$$
{\bf E}_{\theta}\left(\widehat{\theta}(\vec{X}-g(\theta)\right)^2 = {\bf D}_{\theta} \widehat{\theta}(\vec{X}) + \left({\bf E}_{\theta}\widehat{\theta}(\vec{X})-g(\theta)\right)^2
$$
Таким образом, квадратичное отклонение оценки есть сумма квадрата смещения и дисперсии оценки.
\Exam Оценка $\overline{X}$ как оценка ${\bf E}_P X$ имеет квадратичное отклонение
$$
{\bf D}_P \overline{X} = \frac{1}{n^2} {\bf D}_P \left(X_1 + \dotsb + {\bf D}_P X_n\right) = \frac{1}{n} {\bf D}_P X_1.
$$
Оценка $(X_1+\dotsb+X_n+1)/(n+2)$ имеет квадратичное отклонение
$$
{\bf D}_P\frac{X_1+\dotsb+X_n}{n+2} + \left({\bf E}_P\frac{X_1+\dotsb+X_n+1}{n+2}-{\bf E}_P X_1\right)^2 = 
\frac{n{\bf D}_P X_1}{(n+2)^2}+\frac{1}{(n+2)^2} (1 -2{\bf E}_PX_1)^2.
$$
\begin{Def}
Оценка $\widehat{\theta}$ называется оптимальной в классе $\mathcal{E}$ оценок, если она несмещенная и 
$$
{\bf D}_{\theta}\widehat{\theta}(\vec{X})\le {\bf D}_{\theta}\widetilde{\theta}(\vec{X}),\quad
\widetilde{\theta}(\vec{X})\in E, \theta\in\Theta.
$$
\end{Def}
Оптимальная оценка имеет наименьшую дисперсию среди всех оценок из $\mathcal{E}$. В дальнейшем мы научимся строить оптимальные оценки в классе всех несмещенных оценок, пока же мы можем строить оптимальные оценки в небольших классах оценок, напрямую сравнивая дисперсии. 
\Exam Рассмотрим оценки вида $T_{a,b} = a\overline{X}+b\overline{Y}$, где $X_i$, $i\le n$,-- н.о.р. $\mathcal{N}(\theta,\sigma^2)$, $Y_i$, $i\le m$, -- н.о.р. $\mathcal{N}(\theta,2\sigma^2)$. Найдем в этом классе оптимальную оценку $\theta$. Поскольку 
$$
{\bf E}_{\theta,\sigma^2} T_{a,b} = (a+b)\theta,\quad {\bf D}_{\theta,\sigma^2} T_{a,b} = \left(\frac{a^2}{n}+\frac{2b^2}{m}\right)\sigma^2.
$$
Таким образом, несмещенная оценка соответствует $a+b=1$, при этом условии мы должны минимизировать $a^2/n+2b^2/m$. Составляя функцию Лагранжа, получаем
\begin{eqnarray*}
L = \frac{a^2}{n}+\frac{2b^2}{m}+\lambda(a+b-1)\to \min, \quad
\frac{\partial L}{\partial a}:\ \frac{2a}{n}+\lambda = 0,\\
\frac{\partial L}{\partial b}:\ \frac{4b}{n}+\lambda = 0,\quad 
\frac{\partial L}{\partial \lambda}: (a+b-1)=0.
\end{eqnarray*}
Отсюда $a=2/3$, $b=1/3$ дают оптимальную оценку. Строго говоря, требуется проверить, что это действительно минимум, однако других кандидатов на роль наименьшего значения не наблюдается, поскольку на границе функция стремится к бесконечности.
\section{Факультатив}
\subsection{Уравнение несмещенности}
Вопрос поиска (и существования) несмещенной оценки для заданной функции сводится к наличию решения $f(x_1,...,x_n)$ у уравнения несмещенности: равенстве при всех $\theta$ функций
$$
\sum_{x_1,...,x_n} f_{\theta}(x_1)...f_{\theta}(x_n) f(x_1,...,x_n) = g(\theta)$$
в дискретном случае ($f_{\theta}(x) = {\bf P}_{\theta}(X_1=x)$) или
$$
\int_{\mathbb{R}^n} f_{\theta}(x_1)...f_{\theta}(x_n) f(x_1,...,x_n) dx_1 ... dx_n = g(\theta)
$$
в абсолютно-непрерывном случае ($f_{\theta}(x)$ --- плотность). Аналогичное уравнение составляется в случае, если мы хотим построить оценку, являющуюся функцией от заданной статистики $T$, в этом случае мы будем умножать $f(t)$ на плотность или распределение статистики $T$.

Решение такого рода функциональных уравнений оказывается достаточно сложным фактом, однако, зачастую мы можем основываться на разложении функции в ряд Тейлора или поиске ее обратного преобразования Лапласа. Может быть полезной также теорема единственности:
\begin{Th}[теорема единственности для преобразования Лапласа]
Пусть 
$$
\int_{0}^{\infty} f_1(x) e^{-\lambda x} dx = \int_0^{\infty} f_2(x) e^{-\lambda x} dx
$$
при всех $\lambda>0$. Тогда $f_1(x)=f_2(x)$ п.в.
\end{Th}
\Exam Пусть $X\sim \exp(\theta)$. Тогда для любой функции $a(\theta)$ существует не более одной несмещенной оценки от одного наблюдения. Действительно, заметим, что
$$
{\bf E}f(X) = \int_0^{\infty} f(x) \theta e^{-\theta x} dx = a(\theta), 
$$
откуда $f$ есть обратное преобразование Лапласа от $\theta^{-1} a(\theta)$, которое либо существует и единственно, либо не существует. Например, для $a(\theta)=1/\theta$ подходящей функцией будет $f(x)=x$, причем в силу предыдущих утверждений она единственна (с точностью до изменения на множестве меры 0).
\subsection{Общий непараметрический подход к оцениванию}
Как построить оценку в непараметрическом случае? Для этого представим оцениваемый параметр в виде $h(F)$, где $F$ -- ф.р. наблюдений. Тогда можно использовать так называемую естественную оценку $h(\widehat{F}_n)$, где $\widehat{F}_n$ -- ЭФР. 

Скажем, для оценки ${\bf E}X = \int_{-\infty}^{\infty} x dF(x)$ мы используем
$$
\int_{-\infty}^{\infty} x d\widehat{F}_n(x;x_1,\dotsc,x_n) = \sum_{i=1}^{n} x_i \cdot \frac1n = \overline{x},
$$
где мы воспользовались тем, что $\widehat{F}_n$ как функция $x$ есть ф.р. дискретной случайной величины, равновероятно принимающей значения $x_1,\dotsc, x_n$. 

Аналогичным образом для дисперсии естественной оценкой будет $S^2$.

Можно использовать этот подход и в параметрическом случае, однако, здесь будет больше свободы для задания $\theta$ в виде $h(F)$. Так у $\mathcal{N}(\theta,1)$ параметр среднего является и математическим ожиданием, и, например, медианой распределение, что даст различные оценки.

В достаточно широких условиях можно показать, что естественные оценки обладают рядом сильных качеств. К сожалению, несмещенность не входит в их число.
\end{document}


Ниже при упоминании математического ожидания и дисперсии одного наблюдения считается, что они существуют и конечны.\\
В задачах 1.1.0, 1.3.0, 1.3.1, 1.2.2 считаем $\Theta = \mathbb{R}$, 1.2.0, 1.1.2, 1.3.2, 1.1.3, 1.2.3, 1.3.3 $\Theta=\mathbb{R}^+$, в задачах 1.1.1, 1.2.1 $\Theta = [0,1]$.\\
\noindent
{\bf 1.1.0} $X_i\sim \mathcal{N}(\theta,1)$. Построить несмещенную оценку для $\theta^2$.\\
{\bf 1.2.0} Найти такое $c$, что оценка $c X_{(n)}$ несмещенная, $X_i\sim R[0,\theta]$.\\
{\bf 1.3.0} Найти такое $c$, что $X_{(1)}-c$ несмещенная, $f_{\theta}(x) = e^{\theta-x} I_{x>\theta}$.\\
%поменять
\noindent
{\bf 1.1.1} Доказать, что по $X_1$,...,$X_n$ $\sim Bernoulli(\theta)$ нельзя построить несмещенную оценку для $\arccos \theta$.\\
{\bf 1.2.1} $X_i\sim Geom(\theta)$ ($P(X=k) = (1-\theta)^{k} \theta$). Построить несмещенную оценку для $\theta$.\\
%очень просто
{\bf 1.3.1} Исследовать $S^2 = n^{-1} \sum_{i=1}^{n} (X_i-\overline{X})^2$ на несмещенность как оценку дисперсии ${\bf D}_{\theta} X_1$. {\it Указание.} Показать, что $S^2 = \overline{X^2} - \overline{X}^2$. \\
{\bf 1.1.2} $X_i\sim R[0,\theta]$. Построить несмещенную оценку для $(e^{\theta}-1)/\theta$.\\
%Усложнить
{\bf 1.2.2} Построить несмещенную оценку вида $\overline{X}+X_{(1)}-c$ для $2\theta$ по $f_{\theta}(x) = e^{\theta-x} I_{x>\theta}$.\\
{\bf 1.3.2} $X_i\sim \mathcal{N}(a,\theta^2)$, $i=1,...,2n$. Найти несмещенную оценку вида $C \sum_{i=1}^{n} |X_{2i}-X_{2i+1}|$ для $\theta$.\\
{\bf 1.1.3} Построить несмещенную оценку для а) $\theta^k$, б) $1/(1+\theta)$ на основе одного наблюдения $Poiss(\theta)$.\\
%$\theta<1$
{\bf 1.2.3} Построить несмещенную оценку для а) $\theta^{-k}$, б) $e^{1/\theta}$ на основе одного наблюдения $\exp(\theta)$.\\
{\bf 1.3.3} Найти несмещенную оценку с наименьшей дисперсией среди $\alpha X_{(1)}+\beta X_{(n)}$, $X_i\sim R[\theta,2\theta]$. {\it Указание.} ${\bf P}(x<X_{(1)}<X_{(n)}<y)$ легко выразить через $X_i$.
\end{document}
%Построить несмещенную оценку для ${\bf E}_{\theta} X_1$ вида $c_1 X_1 + ... +c_n X_n$ с наименьшей дисперсией .\\
%Несмещенную оценку для среднего и размаха на основе $\min X_i$, $\max X_i$, $R[\theta_1,\theta_2]$.
%{\bf 1.1.3.} Пусть у нас имеется одно наблюдение биномиального закона с параметрами $(k, \theta)$ и параметры $r,s\in \mathbb{N}$. Доказать, что при $r+s\leq k$  оценка $\frac{X(X-1)...(X-r+1)(k-X)(k-X-1)...(k-X-s+1)}{k(k-1)...(k-r-s+1)}$ будет несмещенной оценкой $\theta^r(1-\theta)^s$. Доказать, что при $r+s>k$ несмещенной оценки не существует.\\
%{\bf 1.3.3.} Пусть у нас $X_1$: ${\bf P}(X_1=k)=\frac{e^{-\theta}}{1-e^{-\theta}}\theta^k/k!$, $k>0$. Найти все несмещенные оценки $1-e^{-\theta}$.
